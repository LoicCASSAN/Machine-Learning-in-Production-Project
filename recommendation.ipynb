{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import ast\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "# from data_db import user, mdp\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_recommendation_system(filtered_df):\n",
    "    print(\"Training recommendation system start...\")\n",
    "    start_time = time.time()\n",
    "    # filtered_df = pd.read_csv('Dataset/Book_Dataset.csv')\n",
    "\n",
    "    # def get_user_data(user_email):\n",
    "    #     # Connect to the database\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project') # Change the password accordingly !!!!\n",
    "\n",
    "    #     # Load user's read and liked books from the database\n",
    "    #     query = text(f\"SELECT * FROM user WHERE email = '{user_email}';\")\n",
    "    #     user_data = pd.read_sql_query(query, engine)\n",
    "    #     return user_data\n",
    "    # # user_data = get_user_data('john.doe@example.com')\n",
    "    # def get_user_books(user_email):\n",
    "    #     \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    #     user_books_query = text(f\"SELECT * FROM book WHERE owner = '{user_email}';\")\n",
    "    #     user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #     #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    #     # Prendre en compte la casse et les espaces supplémentaires\n",
    "    #     user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    #     return user_books\n",
    "\n",
    "    # # user_books = get_user_books('john.doe@example.com')\n",
    "    # def get_all_user_books():\n",
    "    #     \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    #     user_books_query = text(f\"SELECT * FROM book;\")\n",
    "    #     user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #     #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    #     # Prendre en compte la casse et les espaces supplémentaires\n",
    "    #     user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    #     return user_books\n",
    "\n",
    "    # all_user_books = get_all_user_books()\n",
    "    # all_user_books.head(1)\n",
    "    # filtered_df.head(1)\n",
    "    # # Grouper par titre et obtenir le premier ProductId pour chaque groupe\n",
    "    # product_ids_by_title = filtered_df.groupby('title')['ProductId'].first()\n",
    "    # # Sélection et renommage des colonnes dans all_user_books pour correspondre à filtered_df\n",
    "    # adapted_all_user_books = all_user_books[['title', 'owner', 'author', 'rating']].copy()\n",
    "    # adapted_all_user_books.rename(columns={'owner': 'UserId', 'author': 'authors', 'rating': 'Score'}, inplace=True)\n",
    "\n",
    "    # # Ajout des colonnes manquantes avec des valeurs par défaut ou vides\n",
    "    # adapted_all_user_books['ProductId'] = ''  # Ajout d'une colonne ProductId vide\n",
    "    # adapted_all_user_books['Time'] = None  # Vous pouvez remplacer None par une valeur par défaut si nécessaire\n",
    "    # adapted_all_user_books['categories'] = None  # Vous pouvez remplacer None par une valeur par défaut si nécessaire\n",
    "\n",
    "    # # Réorganiser les colonnes pour qu'elles correspondent à celles de filtered_df\n",
    "    # adapted_all_user_books = adapted_all_user_books[['ProductId', 'UserId', 'title', 'Score', 'Time', 'authors', 'categories']]\n",
    "\n",
    "    # # Concaténation de adapted_all_user_books avec filtered_df\n",
    "    # filtered_df = pd.concat([filtered_df, adapted_all_user_books], ignore_index=True)\n",
    "\n",
    "    from fuzzywuzzy import process\n",
    "\n",
    "    def find_closest_title(title, titles_list):\n",
    "        closest_title, score = process.extractOne(title, titles_list)\n",
    "        return closest_title if score > 90 else None  # Vous pouvez ajuster le seuil de score\n",
    "\n",
    "    # Parcourir filtered_df pour trouver et associer les ProductId manquants\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        if pd.isnull(row['ProductId']) or row['ProductId'] == '':\n",
    "            closest_title = find_closest_title(row['title'], product_ids_by_title.index)\n",
    "            if closest_title:\n",
    "                filtered_df.at[index, 'ProductId'] = product_ids_by_title[closest_title]\n",
    "\n",
    "    filtered_df = filtered_df.sample(frac=1, random_state=42)\n",
    "    # Get unique UserIds and ProductIds\n",
    "    unique_user_ids = filtered_df['UserId'].unique()\n",
    "    unique_product_ids = filtered_df['ProductId'].unique() #unique ids for books are less\n",
    "\n",
    "    user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "    product_id_to_index = {product_id: index for index, product_id in enumerate(unique_product_ids)}\n",
    "\n",
    "    # clean matrix\n",
    "    matrix = np.zeros((len(unique_user_ids), len(unique_product_ids)))\n",
    "\n",
    "    # users as rows, books as columns with their ratings\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        user_id = row['UserId']\n",
    "        product_id = row['ProductId']\n",
    "        score = row['Score']\n",
    "        \n",
    "        user_index = user_id_to_index[user_id]\n",
    "        product_index = product_id_to_index[product_id]\n",
    "        \n",
    "        if matrix[user_index][product_index] < score:\n",
    "            matrix[user_index][product_index] = score\n",
    "    print(matrix.shape)\n",
    "    matrix\n",
    "    # Z-Scoring\n",
    "    matrix = stats.zscore(matrix, axis=0)\n",
    "    # Evaluation\n",
    "    def calculate_mse(predicted_matrix, test_matrix):\n",
    "        num_users = min(predicted_matrix.shape[0], test_matrix.shape[0])\n",
    "        num_items = min(predicted_matrix.shape[1], test_matrix.shape[1])\n",
    "        mse = np.mean((predicted_matrix[:num_users, :num_items] - test_matrix[:num_users, :num_items]) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def calculate_f1_score(recall, precision):\n",
    "        if recall + precision == 0:\n",
    "            return 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score\n",
    "\n",
    "    def precision_at_k(actual_matrix, predicted_matrix, k, threshold):\n",
    "        binary_predicted_matrix = predicted_matrix >= threshold\n",
    "        \n",
    "        precision = []\n",
    "        for i in range(len(actual_matrix)):\n",
    "            actual_indices = np.where(actual_matrix[i] >= threshold)[0]\n",
    "            predicted_indices = np.argsort(~binary_predicted_matrix[i])[:k]\n",
    "            common_indices = np.intersect1d(actual_indices, predicted_indices)\n",
    "            precision.append(len(common_indices) / len(predicted_indices))\n",
    "        \n",
    "        return np.mean(precision)\n",
    "\n",
    "    def recall_at_k(true_matrix, pred_matrix, k, threshold):\n",
    "        pred_matrix_sorted = np.argsort(pred_matrix, axis=1)[:, ::-1][:, :k]\n",
    "        recall_scores = []\n",
    "        for i in range(len(true_matrix)):\n",
    "            true_positives = len(set(pred_matrix_sorted[i]).intersection(set(np.where(true_matrix[i] >= threshold)[0])))\n",
    "            actual_positives = len(np.where(true_matrix[i] >= threshold)[0])\n",
    "            if actual_positives > 0:\n",
    "                recall_scores.append(true_positives / actual_positives)\n",
    "        recall = np.mean(recall_scores)\n",
    "        return recall\n",
    "    # SVD\n",
    "    def split_train_test(matrix, test_size=0.2, random_state=42):\n",
    "        train_matrix, test_matrix = train_test_split(matrix, test_size=test_size, random_state=random_state)\n",
    "        return train_matrix, test_matrix\n",
    "\n",
    "    def calculate_svd(train_matrix, k=600):\n",
    "        train_sparse = csr_matrix(train_matrix)\n",
    "        # Perform SVD on the sparse matrix\n",
    "        U_train, S_train, VT_train = svds(train_sparse, k=k)\n",
    "        # Reverse the singular values, columns of U_train, and rows of VT_train\n",
    "        S_train_k = np.diag(S_train[::-1])\n",
    "        U_train_k = U_train[:, ::-1]\n",
    "        VT_train_k = VT_train[::-1, :]\n",
    "        \n",
    "        return U_train_k, S_train_k, VT_train_k\n",
    "\n",
    "    train_matrix, test_matrix = split_train_test(matrix)\n",
    "\n",
    "    # training set\n",
    "    U_train, S_train, VT_train = calculate_svd(train_matrix)\n",
    "    U_train_pred = np.dot(train_matrix, VT_train.T)\n",
    "    train_pred_matrix = np.dot(U_train_pred, VT_train)\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    U_test_pred = np.dot(test_matrix, VT_train.T)\n",
    "    predicted_matrix = np.dot(U_test_pred, VT_train)\n",
    "\n",
    "    # Calculate MSE \n",
    "    train_mse = calculate_mse(train_matrix, train_pred_matrix)\n",
    "    test_mse = calculate_mse(test_matrix, predicted_matrix)\n",
    "\n",
    "    print(\"Train Set Mean Squared Error (MSE):\", train_mse)\n",
    "    print(\"Test Set Mean Squared Error (MSE):\", test_mse)\n",
    "    # Calculate Precision at k for the test set\n",
    "    precision = precision_at_k(test_matrix, predicted_matrix, k=10, threshold=3)\n",
    "\n",
    "    # Calculate Recall at k for the test set\n",
    "    recall = recall_at_k(test_matrix, predicted_matrix, k=10, threshold=3)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = calculate_f1_score(recall, precision)\n",
    "    print(\"RMSE (training): \", np.sqrt(train_mse) )\n",
    "    print(\"RMSE (test): \", np.sqrt(test_mse))\n",
    "    print(\"Precision @ 10: \", precision)\n",
    "    print(\"Recall @ 10:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    # Save Model\n",
    "    # Enregistrement des matrices U, S, et VT\n",
    "    def save_with_unique_name(path, data):\n",
    "        dir_name, file_name = os.path.split(path)\n",
    "        base, ext = os.path.splitext(file_name)\n",
    "        counter = 1\n",
    "        new_path = os.path.join(dir_name, f\"{base}{ext}\")  # Définir le chemin sans préfixe de compteur pour le premier essai\n",
    "        while os.path.exists(new_path):  # Vérifier si le fichier existe sans préfixe de compteur\n",
    "            new_path = os.path.join(dir_name, f\"{counter}_{base}{ext}\")  # Ajouter un préfixe de compteur si nécessaire\n",
    "            counter += 1\n",
    "        with open(new_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        return new_path  # Retourner le nouveau chemin pour confirmer où le fichier a été sauvegardé\n",
    "\n",
    "    # Utilisation de la fonction\n",
    "    save_with_unique_name('Model/U_matrix.pkl', U_train)\n",
    "    save_with_unique_name('Model/S_matrix.pkl', S_train)\n",
    "    save_with_unique_name('Model/VT_matrix.pkl', VT_train)\n",
    "    save_with_unique_name('Model/user_id_to_index.pkl', user_id_to_index)\n",
    "    save_with_unique_name('Model/product_id_to_index.pkl', product_id_to_index)\n",
    "    save_with_unique_name('Model/original_matrix.pkl', matrix)\n",
    "    save_with_unique_name('Model/U_train.pkl', U_train)\n",
    "    save_with_unique_name('Model/VT_train.pkl', VT_train)\n",
    "    save_with_unique_name('Model/Book_Dataset.pkl', filtered_df)\n",
    "    \n",
    "    print(\"Recommendation system trained in %s seconds.\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_recommendation_system(filtered_df = pd.read_csv('Dataset/Book_Dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ONLY FOR TESTING\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "def save_with_unique_name(path, data):\n",
    "    dir_name, file_name = os.path.split(path)\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    counter = 1\n",
    "    new_path = os.path.join(dir_name, f\"{base}{ext}\")  # Définir le chemin sans préfixe de compteur pour le premier essai\n",
    "    while os.path.exists(new_path):  # Vérifier si le fichier existe sans préfixe de compteur\n",
    "        new_path = os.path.join(dir_name, f\"{counter}_{base}{ext}\")  # Ajouter un préfixe de compteur si nécessaire\n",
    "        counter += 1\n",
    "    with open(new_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    return new_path  # Retourner le nouveau chemin pour confirmer où le fichier a été sauvegardé\n",
    "\n",
    "# Utilisation de la fonction\n",
    "save_with_unique_name('Model/U_matrix.pkl', U_train)\n",
    "save_with_unique_name('Model/S_matrix.pkl', S_matrix)\n",
    "save_with_unique_name('Model/VT_matrix.pkl', VT_train)\n",
    "save_with_unique_name('Model/user_id_to_index.pkl', user_id_to_index)\n",
    "save_with_unique_name('Model/product_id_to_index.pkl', product_id_to_index)\n",
    "save_with_unique_name('Model/original_matrix.pkl', original_matrix)\n",
    "save_with_unique_name('Model/U_train.pkl', U_train)\n",
    "save_with_unique_name('Model/VT_train.pkl', VT_train)\n",
    "save_with_unique_name('Model/Book_Dataset.pkl', filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def get_latest_file(path_pattern):\n",
    "    dir_name, file_pattern = os.path.split(path_pattern)\n",
    "    base, ext = os.path.splitext(file_pattern)\n",
    "    base = base.replace('*', '')  # Supprimez l'astérisque pour la correspondance\n",
    "    files = [f for f in os.listdir(dir_name) if f.endswith(base + ext) and '_' in f]\n",
    "    if not files:  # S'il n'y a pas de fichier correspondant\n",
    "        raise FileNotFoundError(f\"No file found for the pattern {path_pattern}\")\n",
    "    # Trier les fichiers en fonction du numéro avant le tiret bas, s'il existe, sinon 0\n",
    "    latest_file = max(files, key=lambda x: int(x.split('_')[0]) if x.split('_')[0].isdigit() else 0)\n",
    "    return os.path.join(dir_name, latest_file)\n",
    "\n",
    "def load_model_and_mappings():\n",
    "    U_matrix_path = get_latest_file('Model/*U_matrix.pkl')\n",
    "    S_matrix_path = get_latest_file('Model/*S_matrix.pkl')\n",
    "    VT_matrix_path = get_latest_file('Model/*VT_matrix.pkl')\n",
    "    user_id_to_index_path = get_latest_file('Model/*user_id_to_index.pkl')\n",
    "    product_id_to_index_path = get_latest_file('Model/*product_id_to_index.pkl')\n",
    "    original_matrix_path = get_latest_file('Model/*original_matrix.pkl')\n",
    "    U_train_path = get_latest_file('Model/*U_train.pkl')\n",
    "    VT_train_path = get_latest_file('Model/*VT_train.pkl')\n",
    "    Book_Dataset_path = get_latest_file('Model/*Book_Dataset.pkl')\n",
    "\n",
    "    with open(U_matrix_path, 'rb') as f:\n",
    "        U_matrix = pickle.load(f)\n",
    "    with open(S_matrix_path, 'rb') as f:\n",
    "        S_matrix = pickle.load(f)\n",
    "    with open(VT_matrix_path, 'rb') as f:\n",
    "        VT_matrix = pickle.load(f)\n",
    "    with open(user_id_to_index_path, 'rb') as f:\n",
    "        user_id_to_index = pickle.load(f)\n",
    "    with open(product_id_to_index_path, 'rb') as f:\n",
    "        product_id_to_index = pickle.load(f)\n",
    "    with open(original_matrix_path, 'rb') as f:\n",
    "        original_matrix = pickle.load(f)\n",
    "    with open(U_train_path, 'rb') as f:\n",
    "        U_train = pickle.load(f)\n",
    "    with open(VT_train_path, 'rb') as f:\n",
    "        VT_train = pickle.load(f)\n",
    "    filtered_df = pd.read_pickle(Book_Dataset_path)\n",
    "\n",
    "    return U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df\n",
    "\n",
    "# Utilisation de la fonction\n",
    "try:\n",
    "    U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>title</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123043</th>\n",
       "      <td>1557424470</td>\n",
       "      <td>A3IO5DHSA7XU8S</td>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1308528000</td>\n",
       "      <td>Óscar Wilde</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149547</th>\n",
       "      <td>B000NWQXBA</td>\n",
       "      <td>A3RK5D0VVDGCPP</td>\n",
       "      <td>the hobbit</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1296345600</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>Juvenile Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413964</th>\n",
       "      <td>B00086U8GW</td>\n",
       "      <td>A1XMUOTUET9TOV</td>\n",
       "      <td>the sun also rises (the modern library of the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1112745600</td>\n",
       "      <td>Ernest Hemingway</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151232</th>\n",
       "      <td>0736605010</td>\n",
       "      <td>A36DJ9VIOIUG6K</td>\n",
       "      <td>wuthering heights</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1357776000</td>\n",
       "      <td>Emily Bronte</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294384</th>\n",
       "      <td>B000AYI0JY</td>\n",
       "      <td>A1UI7DNTDJRCI0</td>\n",
       "      <td>moby-dick or the whale</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1039132800</td>\n",
       "      <td>Herman Melville</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>8188280046</td>\n",
       "      <td>A3CHXQLCBO7S9Y</td>\n",
       "      <td>pride &amp; prejudice (classic library)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1360022400</td>\n",
       "      <td>Ibi Zoboi</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>B0006CBNKI</td>\n",
       "      <td>AME10MRDYP6V7</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1207353600</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365838</th>\n",
       "      <td>0708910351</td>\n",
       "      <td>ACTEKUQIV8SKE</td>\n",
       "      <td>chariots of the gods</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1357776000</td>\n",
       "      <td>Erich von Diken</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>B0000B194H</td>\n",
       "      <td>AFITDZEWKJKAM</td>\n",
       "      <td>it's not about the bike: my journey back to life</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1066694400</td>\n",
       "      <td>Lance Armstrong</td>\n",
       "      <td>Biography &amp; Autobiography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>B0007AV14W</td>\n",
       "      <td>A2SN81RKUIAGSI</td>\n",
       "      <td>till we have faces: a myth retold</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1358035200</td>\n",
       "      <td>C. S. Lewis</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534135 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductId          UserId  \\\n",
       "123043  1557424470  A3IO5DHSA7XU8S   \n",
       "149547  B000NWQXBA  A3RK5D0VVDGCPP   \n",
       "413964  B00086U8GW  A1XMUOTUET9TOV   \n",
       "151232  0736605010  A36DJ9VIOIUG6K   \n",
       "294384  B000AYI0JY  A1UI7DNTDJRCI0   \n",
       "...            ...             ...   \n",
       "110268  8188280046  A3CHXQLCBO7S9Y   \n",
       "259178  B0006CBNKI   AME10MRDYP6V7   \n",
       "365838  0708910351   ACTEKUQIV8SKE   \n",
       "131932  B0000B194H   AFITDZEWKJKAM   \n",
       "121958  B0007AV14W  A2SN81RKUIAGSI   \n",
       "\n",
       "                                                    title  Score        Time  \\\n",
       "123043                         the picture of dorian gray    2.0  1308528000   \n",
       "149547                                         the hobbit    5.0  1296345600   \n",
       "413964  the sun also rises (the modern library of the ...    2.0  1112745600   \n",
       "151232                                  wuthering heights    1.0  1357776000   \n",
       "294384                             moby-dick or the whale    1.0  1039132800   \n",
       "...                                                   ...    ...         ...   \n",
       "110268                pride & prejudice (classic library)    5.0  1360022400   \n",
       "259178                              to kill a mockingbird    5.0  1207353600   \n",
       "365838                               chariots of the gods    2.0  1357776000   \n",
       "131932   it's not about the bike: my journey back to life    4.0  1066694400   \n",
       "121958                  till we have faces: a myth retold    5.0  1358035200   \n",
       "\n",
       "                 authors                 categories  \n",
       "123043       Óscar Wilde                        NaN  \n",
       "149547  J. R. R. Tolkien           Juvenile Fiction  \n",
       "413964  Ernest Hemingway                        NaN  \n",
       "151232      Emily Bronte                        NaN  \n",
       "294384   Herman Melville                    Fiction  \n",
       "...                  ...                        ...  \n",
       "110268         Ibi Zoboi        Young Adult Fiction  \n",
       "259178        Harper Lee                    Fiction  \n",
       "365838   Erich von Diken                    History  \n",
       "131932   Lance Armstrong  Biography & Autobiography  \n",
       "121958       C. S. Lewis                    Fiction  \n",
       "\n",
       "[534135 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SUR UTILISATEUR DEJA PRÉSENT\n",
    "def fetch_relevant_items_for_user(user_id, relevant_items=35):\n",
    "    # Get the index of the user\n",
    "    U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()\n",
    "    user_index = user_id_to_index[user_id]\n",
    "    user_embedding = U_train[user_index, :]\n",
    "    \n",
    "    similarity_scores = VT_train.T.dot(user_embedding)\n",
    "\n",
    "    sorted_indices = similarity_scores.argsort()[::-1]\n",
    "    top_relevant_indices = sorted_indices[:relevant_items]\n",
    "    \n",
    "    relevant_items = [list(product_id_to_index.keys())[list(product_id_to_index.values()).index(idx)] for idx in top_relevant_indices]\n",
    "    relevant_titles = filtered_df.loc[filtered_df['ProductId'].isin(relevant_items), 'title'].tolist()\n",
    "    \n",
    "    # Remove any duplicate titles\n",
    "    unique_relevant_titles = list(set(relevant_titles))\n",
    "    \n",
    "    # Get the final set of relevant items without duplicate titles\n",
    "    final_relevant_items = []\n",
    "    for title in unique_relevant_titles:\n",
    "        final_relevant_items.append(title)\n",
    "    \n",
    "    return final_relevant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations_for_user(user_id, top_n=35):\n",
    "    relevant_items = fetch_relevant_items_for_user(user_id, top_n)\n",
    "    relevant_items_df = pd.DataFrame(relevant_items, columns=['title'])\n",
    "    return relevant_items_df\n",
    "\n",
    "# Utilisation de la fonction\n",
    "user_id = \"A36DJ9VIOIUG6K\"\n",
    "top_n_recommendations = 35\n",
    "\n",
    "test = provide_recommendations_for_user(user_id, top_n_recommendations)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations_for_user(user_id, top_n=35, filtered_df=filtered_df):\n",
    "    relevant_items = fetch_relevant_items_for_user(user_id, top_n)\n",
    "    relevant_items_df = filtered_df[filtered_df['title'].isin(relevant_items)]\n",
    "    relevant_items_df = relevant_items_df.drop('UserId', axis=1)\n",
    "    \n",
    "    relevant_items_df = relevant_items_df.rename(columns={'Score': 'Predict_Score'})\n",
    "    relevant_items_df = relevant_items_df.sort_values(by='Predict_Score', ascending=False)\n",
    "\n",
    "    # Grouper par titre et prendre la première occurrence de chaque titre\n",
    "    relevant_items_df = relevant_items_df.groupby('title').first().reset_index()\n",
    "\n",
    "    return relevant_items_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Utilisation de la fonction\n",
    "user_id = \"A36DJ9VIOIUG6K\"\n",
    "top_n_recommendations = 35\n",
    "\n",
    "test = provide_recommendations_for_user(user_id, top_n_recommendations)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_user_info(user_id, df):\n",
    "    user_info = df.loc[df['UserId'] == user_id]\n",
    "    return user_info\n",
    "# Utilisation de la fonction\n",
    "test = display_user_info('A36DJ9VIOIUG6K', filtered_df)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_books(user_email):\n",
    "    \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    user_books_query = text(f\"SELECT * FROM book WHERE owner = '{user_email}';\")\n",
    "    user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    # Prendre en compte la casse et les espaces supplémentaires\n",
    "    user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    print(\"User book :\",user_books)\n",
    "    return user_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations_for_user(user_id, top_n=35):\n",
    "    U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()\n",
    "    print(\"Recommendation for user\", user_id)\n",
    "    # Fetch relevant items for the user\n",
    "    relevant_items = fetch_relevant_items_for_user(user_id, top_n)\n",
    "\n",
    "    # Create a list to store the recommendations\n",
    "    recommendations = [(title, user_id, '', '', '', '') for title in relevant_items]\n",
    "\n",
    "    # Connect to the database\n",
    "    engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        # Delete old recommendations for this user\n",
    "        delete_query = text(\"DELETE FROM recco_book WHERE owner = :owner\")\n",
    "        connection.execute(delete_query, {\"owner\": user_id})\n",
    "\n",
    "        # Insert new recommendations\n",
    "        insert_query = text(\"INSERT INTO recco_book (title, owner, author, year, type, publisher) VALUES (:title, :owner, :author, :year, :type, :publisher)\")\n",
    "        for rec in recommendations:\n",
    "            connection.execute(insert_query, {\"title\": rec[0], \"owner\": rec[1], \"author\": rec[2], \"year\": rec[3], \"type\": rec[4], \"publisher\": rec[5]})\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "\n",
    "    # Return the list of recommendations (optional)\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_book(row, df):\n",
    "    new_df = pd.DataFrame(row, index=[0])\n",
    "    return pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Utilisation de la fonction\n",
    "new_row = {'ProductId': 'test', 'UserId':'test', 'title': 'New Product', 'Score': 5, 'Time': '2022-01-01', 'authors': 'Author Name', 'categories': 'Category Name'}\n",
    "filtered_df = add_book(new_row, filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = display_user_info('test', filtered_df)\n",
    "test.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
