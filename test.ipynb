{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import ast\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "# from data_db import user, mdp\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_recommendation_system():\n",
    "    print(\"Training recommendation system start...\")\n",
    "    start_time = time.time()\n",
    "    filtered_df = pd.read_csv('Dataset/Book_Dataset.csv', nrows=10000000)\n",
    "\n",
    "    # def get_user_data(user_email):\n",
    "    #     # Connect to the database\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project') # Change the password accordingly !!!!\n",
    "\n",
    "    #     # Load user's read and liked books from the database\n",
    "    #     query = text(f\"SELECT * FROM user WHERE email = '{user_email}';\")\n",
    "    #     user_data = pd.read_sql_query(query, engine)\n",
    "    #     return user_data\n",
    "    # # user_data = get_user_data('john.doe@example.com')\n",
    "    # def get_user_books(user_email):\n",
    "    #     \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    #     user_books_query = text(f\"SELECT * FROM book WHERE owner = '{user_email}';\")\n",
    "    #     user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #     #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    #     # Prendre en compte la casse et les espaces supplémentaires\n",
    "    #     user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    #     return user_books\n",
    "\n",
    "    # # user_books = get_user_books('john.doe@example.com')\n",
    "    # def get_all_user_books():\n",
    "    #     \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    #     engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    #     user_books_query = text(f\"SELECT * FROM book;\")\n",
    "    #     user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #     #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    #     # Prendre en compte la casse et les espaces supplémentaires\n",
    "    #     user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    #     return user_books\n",
    "\n",
    "    # all_user_books = get_all_user_books()\n",
    "    # all_user_books.head(1)\n",
    "    # filtered_df.head(1)\n",
    "    # # Grouper par titre et obtenir le premier ProductId pour chaque groupe\n",
    "    # product_ids_by_title = filtered_df.groupby('title')['ProductId'].first()\n",
    "    # # Sélection et renommage des colonnes dans all_user_books pour correspondre à filtered_df\n",
    "    # adapted_all_user_books = all_user_books[['title', 'owner', 'author', 'rating']].copy()\n",
    "    # adapted_all_user_books.rename(columns={'owner': 'UserId', 'author': 'authors', 'rating': 'Score'}, inplace=True)\n",
    "\n",
    "    # # Ajout des colonnes manquantes avec des valeurs par défaut ou vides\n",
    "    # adapted_all_user_books['ProductId'] = ''  # Ajout d'une colonne ProductId vide\n",
    "    # adapted_all_user_books['Time'] = None  # Vous pouvez remplacer None par une valeur par défaut si nécessaire\n",
    "    # adapted_all_user_books['categories'] = None  # Vous pouvez remplacer None par une valeur par défaut si nécessaire\n",
    "\n",
    "    # # Réorganiser les colonnes pour qu'elles correspondent à celles de filtered_df\n",
    "    # adapted_all_user_books = adapted_all_user_books[['ProductId', 'UserId', 'title', 'Score', 'Time', 'authors', 'categories']]\n",
    "\n",
    "    # # Concaténation de adapted_all_user_books avec filtered_df\n",
    "    # filtered_df = pd.concat([filtered_df, adapted_all_user_books], ignore_index=True)\n",
    "\n",
    "    from fuzzywuzzy import process\n",
    "\n",
    "    def find_closest_title(title, titles_list):\n",
    "        closest_title, score = process.extractOne(title, titles_list)\n",
    "        return closest_title if score > 90 else None  # Vous pouvez ajuster le seuil de score\n",
    "\n",
    "    # Parcourir filtered_df pour trouver et associer les ProductId manquants\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        if pd.isnull(row['ProductId']) or row['ProductId'] == '':\n",
    "            closest_title = find_closest_title(row['title'], product_ids_by_title.index)\n",
    "            if closest_title:\n",
    "                filtered_df.at[index, 'ProductId'] = product_ids_by_title[closest_title]\n",
    "\n",
    "    filtered_df = filtered_df.sample(frac=1, random_state=42)\n",
    "    # Get unique UserIds and ProductIds\n",
    "    unique_user_ids = filtered_df['UserId'].unique()\n",
    "    unique_product_ids = filtered_df['ProductId'].unique() #unique ids for books are less\n",
    "\n",
    "    user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "    product_id_to_index = {product_id: index for index, product_id in enumerate(unique_product_ids)}\n",
    "\n",
    "    # clean matrix\n",
    "    matrix = np.zeros((len(unique_user_ids), len(unique_product_ids)))\n",
    "\n",
    "    # users as rows, books as columns with their ratings\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        user_id = row['UserId']\n",
    "        product_id = row['ProductId']\n",
    "        score = row['Score']\n",
    "        \n",
    "        user_index = user_id_to_index[user_id]\n",
    "        product_index = product_id_to_index[product_id]\n",
    "        \n",
    "        if matrix[user_index][product_index] < score:\n",
    "            matrix[user_index][product_index] = score\n",
    "    print(matrix.shape)\n",
    "    matrix\n",
    "    # Z-Scoring\n",
    "    matrix = stats.zscore(matrix, axis=0)\n",
    "    # Evaluation\n",
    "    def calculate_mse(predicted_matrix, test_matrix):\n",
    "        num_users = min(predicted_matrix.shape[0], test_matrix.shape[0])\n",
    "        num_items = min(predicted_matrix.shape[1], test_matrix.shape[1])\n",
    "        mse = np.mean((predicted_matrix[:num_users, :num_items] - test_matrix[:num_users, :num_items]) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def calculate_f1_score(recall, precision):\n",
    "        if recall + precision == 0:\n",
    "            return 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score\n",
    "\n",
    "    def precision_at_k(actual_matrix, predicted_matrix, k, threshold):\n",
    "        binary_predicted_matrix = predicted_matrix >= threshold\n",
    "        \n",
    "        precision = []\n",
    "        for i in range(len(actual_matrix)):\n",
    "            actual_indices = np.where(actual_matrix[i] >= threshold)[0]\n",
    "            predicted_indices = np.argsort(~binary_predicted_matrix[i])[:k]\n",
    "            common_indices = np.intersect1d(actual_indices, predicted_indices)\n",
    "            precision.append(len(common_indices) / len(predicted_indices))\n",
    "        \n",
    "        return np.mean(precision)\n",
    "\n",
    "    def recall_at_k(true_matrix, pred_matrix, k, threshold):\n",
    "        pred_matrix_sorted = np.argsort(pred_matrix, axis=1)[:, ::-1][:, :k]\n",
    "        recall_scores = []\n",
    "        for i in range(len(true_matrix)):\n",
    "            true_positives = len(set(pred_matrix_sorted[i]).intersection(set(np.where(true_matrix[i] >= threshold)[0])))\n",
    "            actual_positives = len(np.where(true_matrix[i] >= threshold)[0])\n",
    "            if actual_positives > 0:\n",
    "                recall_scores.append(true_positives / actual_positives)\n",
    "        recall = np.mean(recall_scores)\n",
    "        return recall\n",
    "    # SVD\n",
    "    def split_train_test(matrix, test_size=0.2, random_state=42):\n",
    "        train_matrix, test_matrix = train_test_split(matrix, test_size=test_size, random_state=random_state)\n",
    "        return train_matrix, test_matrix\n",
    "\n",
    "    def calculate_svd(train_matrix, k=600):\n",
    "        train_sparse = csr_matrix(train_matrix)\n",
    "        # Perform SVD on the sparse matrix\n",
    "        U_train, S_train, VT_train = svds(train_sparse, k=k)\n",
    "        # Reverse the singular values, columns of U_train, and rows of VT_train\n",
    "        S_train_k = np.diag(S_train[::-1])\n",
    "        U_train_k = U_train[:, ::-1]\n",
    "        VT_train_k = VT_train[::-1, :]\n",
    "        \n",
    "        return U_train_k, S_train_k, VT_train_k\n",
    "\n",
    "    train_matrix, test_matrix = split_train_test(matrix)\n",
    "\n",
    "    # training set\n",
    "    U_train, S_train, VT_train = calculate_svd(train_matrix)\n",
    "    U_train_pred = np.dot(train_matrix, VT_train.T)\n",
    "    train_pred_matrix = np.dot(U_train_pred, VT_train)\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    U_test_pred = np.dot(test_matrix, VT_train.T)\n",
    "    predicted_matrix = np.dot(U_test_pred, VT_train)\n",
    "\n",
    "    # Calculate MSE \n",
    "    train_mse = calculate_mse(train_matrix, train_pred_matrix)\n",
    "    test_mse = calculate_mse(test_matrix, predicted_matrix)\n",
    "\n",
    "    print(\"Train Set Mean Squared Error (MSE):\", train_mse)\n",
    "    print(\"Test Set Mean Squared Error (MSE):\", test_mse)\n",
    "    # Calculate Precision at k for the test set\n",
    "    precision = precision_at_k(test_matrix, predicted_matrix, k=10, threshold=3)\n",
    "\n",
    "    # Calculate Recall at k for the test set\n",
    "    recall = recall_at_k(test_matrix, predicted_matrix, k=10, threshold=3)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = calculate_f1_score(recall, precision)\n",
    "    print(\"RMSE (training): \", np.sqrt(train_mse) )\n",
    "    print(\"RMSE (test): \", np.sqrt(test_mse))\n",
    "    print(\"Precision @ 10: \", precision)\n",
    "    print(\"Recall @ 10:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    # Save Model\n",
    "    # Enregistrement des matrices U, S, et VT\n",
    "    with open('Model/U_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(U_train, f)\n",
    "\n",
    "    with open('Model/S_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(S_train, f)\n",
    "\n",
    "    with open('Model/VT_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(VT_train, f)\n",
    "\n",
    "    # Enregistrement des mappages\n",
    "    with open('Model/user_id_to_index.pkl', 'wb') as f:\n",
    "        pickle.dump(user_id_to_index, f)\n",
    "\n",
    "    with open('Model/product_id_to_index.pkl', 'wb') as f:\n",
    "        pickle.dump(product_id_to_index, f)\n",
    "\n",
    "    # Enregistrement de la matrice d'origine\n",
    "    with open('Model/original_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(matrix, f)\n",
    "\n",
    "    with open('Model/user_id_to_index.pkl', 'wb') as f:\n",
    "        pickle.dump(user_id_to_index, f)\n",
    "        \n",
    "    with open('Model/U_train.pkl', 'wb') as f:\n",
    "        pickle.dump(U_train, f)\n",
    "        \n",
    "    with open('Model/VT_train.pkl', 'wb') as f:\n",
    "        pickle.dump(VT_train, f)\n",
    "        \n",
    "    filtered_df.to_pickle('Model/books_metadata.pkl')\n",
    "    \n",
    "    print(\"Recommendation system trained in %s seconds.\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training recommendation system start...\n",
      "(33624, 1809)\n",
      "Train Set Mean Squared Error (MSE): 0.10668244465153981\n",
      "Test Set Mean Squared Error (MSE): 0.15933495739581469\n",
      "RMSE (training):  0.32662278648548054\n",
      "RMSE (test):  0.39916783111344867\n",
      "Precision @ 10:  0.645903345724907\n",
      "Recall @ 10: 0.8562981836831944\n",
      "F1 Score: 0.7363670598805256\n",
      "Recommendation system trained in 256.64604020118713 seconds.\n"
     ]
    }
   ],
   "source": [
    "book_recommendation_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_mappings():\n",
    "    with open('Model/U_matrix.pkl', 'rb') as f:\n",
    "        U_matrix = pickle.load(f)\n",
    "\n",
    "    with open('Model/S_matrix.pkl', 'rb') as f:\n",
    "        S_matrix = pickle.load(f)\n",
    "\n",
    "    with open('Model/VT_matrix.pkl', 'rb') as f:\n",
    "        VT_matrix = pickle.load(f)\n",
    "\n",
    "    with open('Model/user_id_to_index.pkl', 'rb') as f:\n",
    "        user_id_to_index = pickle.load(f)\n",
    "\n",
    "    with open('Model/product_id_to_index.pkl', 'rb') as f:\n",
    "        product_id_to_index = pickle.load(f)\n",
    "\n",
    "    with open('Model/original_matrix.pkl', 'rb') as f:\n",
    "        original_matrix = pickle.load(f)\n",
    "\n",
    "    with open('Model/U_train.pkl', 'rb') as f:\n",
    "        U_train = pickle.load(f)\n",
    "        \n",
    "    with open('Model/VT_train.pkl', 'rb') as f:\n",
    "        VT_train = pickle.load(f)\n",
    "        \n",
    "    filtered_df = pd.read_pickle('Model/books_metadata.pkl')\n",
    "\n",
    "    return U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df\n",
    "\n",
    "# Utilisation de la fonction\n",
    "U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SUR UTILISATEUR DEJA PRÉSENT\n",
    "def fetch_relevant_items_for_user(user_id, relevant_items=35):\n",
    "    # Get the index of the user\n",
    "    U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()\n",
    "    user_index = user_id_to_index[user_id]\n",
    "    user_embedding = U_train[user_index, :]\n",
    "    \n",
    "    similarity_scores = VT_train.T.dot(user_embedding)\n",
    "\n",
    "    sorted_indices = similarity_scores.argsort()[::-1]\n",
    "    top_relevant_indices = sorted_indices[:relevant_items]\n",
    "    \n",
    "    relevant_items = [list(product_id_to_index.keys())[list(product_id_to_index.values()).index(idx)] for idx in top_relevant_indices]\n",
    "    relevant_titles = filtered_df.loc[filtered_df['ProductId'].isin(relevant_items), 'title'].tolist()\n",
    "    \n",
    "    # Remove any duplicate titles\n",
    "    unique_relevant_titles = list(set(relevant_titles))\n",
    "    \n",
    "    # Get the final set of relevant items without duplicate titles\n",
    "    final_relevant_items = []\n",
    "    for title in unique_relevant_titles:\n",
    "        final_relevant_items.append(title)\n",
    "    \n",
    "    return final_relevant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations_for_user(user_id, top_n=35):\n",
    "    relevant_items = fetch_relevant_items_for_user(user_id, top_n)\n",
    "    print(f\"User: {user_id}\")\n",
    "    print(\"Relevant Items:\")\n",
    "    for i, item in enumerate(relevant_items):\n",
    "        print(f\"{i+1}. {item}\")\n",
    "    print()\n",
    "\n",
    "# Utilisation de la fonction\n",
    "user_email = \"lc@mail.com\"\n",
    "top_n_recommendations = 35\n",
    "\n",
    "provide_recommendations_for_user(user_email, top_n_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_books(user_email):\n",
    "    \"\"\" Récupère les livres d'un utilisateur à partir de la base de données \"\"\"\n",
    "    engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "    user_books_query = text(f\"SELECT * FROM book WHERE owner = '{user_email}';\")\n",
    "    user_books = pd.read_sql_query(user_books_query, engine)\n",
    "    #user_books = user_books[user_books['rating'].between(0, 5)]\n",
    "    # Prendre en compte la casse et les espaces supplémentaires\n",
    "    user_books['title'] = user_books['title'].str.strip().str.lower()\n",
    "    print(\"User book :\",user_books)\n",
    "    return user_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_recommendations_for_user(user_id, top_n=35):\n",
    "    U_matrix, S_matrix, VT_matrix, user_id_to_index, product_id_to_index, original_matrix, U_train, VT_train, filtered_df = load_model_and_mappings()\n",
    "    print(\"Recommendation for user\", user_id)\n",
    "    # Fetch relevant items for the user\n",
    "    relevant_items = fetch_relevant_items_for_user(user_id, top_n)\n",
    "\n",
    "    # Create a list to store the recommendations\n",
    "    recommendations = [(title, user_id, '', '', '', '') for title in relevant_items]\n",
    "\n",
    "    # Connect to the database\n",
    "    engine = create_engine('mysql+pymysql://'+user+':'+mdp+'@localhost:3306/db_master_project')\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        # Delete old recommendations for this user\n",
    "        delete_query = text(\"DELETE FROM recco_book WHERE owner = :owner\")\n",
    "        connection.execute(delete_query, {\"owner\": user_id})\n",
    "\n",
    "        # Insert new recommendations\n",
    "        insert_query = text(\"INSERT INTO recco_book (title, owner, author, year, type, publisher) VALUES (:title, :owner, :author, :year, :type, :publisher)\")\n",
    "        for rec in recommendations:\n",
    "            connection.execute(insert_query, {\"title\": rec[0], \"owner\": rec[1], \"author\": rec[2], \"year\": rec[3], \"type\": rec[4], \"publisher\": rec[5]})\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "\n",
    "    # Return the list of recommendations (optional)\n",
    "    return recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
